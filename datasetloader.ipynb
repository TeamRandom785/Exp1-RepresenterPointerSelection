{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Softmax function which is similar to Pytorch's Softmax but this Softmax function can calculate the loss\n",
    "#w.r.t loss function and also the L2 regularizer. This is used to train the pretrained model's last layer to converge\n",
    "#completely at its Global Minimum. Also, this Softmax function used LogSumTrick to handle numerical underflow \n",
    "class softmax(nn.Module):\n",
    "    def __init__(self,W):\n",
    "        super(softmax,self).__init__()\n",
    "        self.W = Variable(torch.from_numpy(W).type(dtype),requires_grad=True)\n",
    "     \n",
    "    #Here x is the loss function and y is the L2 regularizer\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        #Calculating the matrix mult between x and W \n",
    "        Y = torch.matmul(x, self.W)\n",
    "        Y_max,_ = torch.max(Y, dim=1, keepdim = True)\n",
    "        Y = Y - Y_max\n",
    "        \n",
    "        tmp_var_a = torch.log(torch.sum(torch.exp(Y),dim = 1))\n",
    "        tmp_var_b = torch.sum(Y*y, dim = 1)\n",
    "        \n",
    "        sigma = torch.sum(tmp_var_a - tmp_var_b)\n",
    "        reg_W = torch.squeeze(self.W)\n",
    "        L2 = torch.sum(torch.mul(reg_W,reg_W))\n",
    "        return (sigma,L2)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax function for numpy variables\n",
    "def softmax_np(x):\n",
    "    e_x = np.exp(x - np.max(x,axis = 1,keepdims = True))\n",
    "    return e_x / e_x.sum(axis = 1,keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    if dataset == \"Cifar\":\n",
    "        \n",
    "        input_file = open(\"Cifar/weight_323436.pkl\", \"rb\")\n",
    "        [W_32,W_34,W_36,intermediate_output_32,intermediate_output_34,intermediate_output_36] = pickle.load(input_file, encoding = 'latin1')\n",
    "        print((softmax_np(np.matmul(np.concatenate([intermediate_output_34,np.ones((intermediate_output_34.shape[0],1))],axis = 1),W_36))-intermediate_output_36)[:5,:])\n",
    "        print(intermediate_output_36[:5,:])\n",
    "        print('done loading')\n",
    "        model = softmax(W_36)\n",
    "        model.cuda()\n",
    "        start = time.time()\n",
    "        return (np.concatenate([intermediate_output_34,np.ones((intermediate_output_34.shape[0],1))],axis = 1), intermediate_output_36, model)\n",
    "    \n",
    "    elif dataset == \"AwA\":\n",
    "       \n",
    "        input_file = open(\"AwA/weight_bias.pickle\", \"rb\")\n",
    "        [weight,bias] = pickle.load(input_file, encoding = 'latin1')\n",
    "        train_feature = np.squeeze(np.load('AwA/train_feature_awa.npy'))\n",
    "        train_output = np.squeeze(np.load('Awa/train_output_awa.npy'))\n",
    "        weight = np.transpose(np.concatenate([weight,np.expand_dims(bias,1)],axis = 1))\n",
    "        train_feature = np.concatenate([train_feature,np.ones((train_feature.shape[0],1))],axis = 1)\n",
    "        train_output = softmax_np(train_output)\n",
    "        model = softmax(weight)\n",
    "        model.cuda()\n",
    "        return (train_feature,train_output,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
